{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chopin Nocturnes - analysing matching subsequences in training and generated data\n",
    "## Comparing Generated Sequences to Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music_generator.serializers.discrete_time_serializer import DiscreteTimeMidiSerializer\n",
    "import music_generator.utilities.sequence_utils as sequence_utils\n",
    "import music_generator.utilities.utils as utils\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up constants for generation and comparison\n",
    "\n",
    "A single quarter-note four-voice chord would be made up of at least 9 events (4 note-on, 4 note-off, at least 4 wait events if no chaining is required),\n",
    "so a measure of four quarter-note four-voice chords would be a minimum of 36 events.\n",
    "\n",
    "Adding in eigth notes, flourishes, etc. would increase the number of events in a measure.\n",
    "\n",
    "Windows of 100 events are used to compare generated sequences and training sequences. This would generally represent somewhere from one to four measures, depending on note frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "serializer = DiscreteTimeMidiSerializer()\n",
    "window_size = 100\n",
    "training_data = './training_data/chopin_nocturnes/'\n",
    "generated_data = './generated_files/chopin_nocturnes_temperature_{}/'\n",
    "temperatures = [1.0, 1.2, 1.5, 2.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a set of all unique sub-sequences with length = window_size from the training data\n",
    "\n",
    "The sequences from the training data are transposed over one octave.\n",
    "\n",
    "These sequences are then windowed to the previously decided number of events.\n",
    "\n",
    "Each window is converted to an equivalent string to make it a hashable object, and then added to a set for comparison with another set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data windows of length 100: 1555488\n",
      "Unique windows: 1482911\n"
     ]
    }
   ],
   "source": [
    "real_sequences = serializer.serialize_folder(training_data)\n",
    "\n",
    "# transpose training data to all keys and window\n",
    "real_sequences = sequence_utils.transpose(real_sequences, down=-6, up=5)\n",
    "real_sequences, _ = sequence_utils.window(real_sequences, window_size=window_size)\n",
    "print('Training data windows of length {}: {}'.format(window_size, len(real_sequences)))\n",
    "\n",
    "real_set = utils.create_hashable_set(real_sequences)\n",
    "print('Unique windows: {}'.format(len(real_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare generated sequences to the training data\n",
    "Sequences generated by the model using different seeds and temperature settings are serialized, converted to a hashable string, added to a set, and then compared to the training data one at a time.\n",
    "The percentage of unique windows in a generated composition that appear exactly in the training data is calculated.\n",
    "The average percentage of 'copied' sequences for an entire set of compositions using a given temperature setting is also calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEMPERATURE: 1.0\n",
      "sample_41-50-57-65-69.mid: 36.92% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_82.mid: 33.21% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_39-48-55-63-67.mid: 35.51% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_80.mid: 78.85% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_40-49-56-64-68.mid: 68.40% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_81.mid: 86.07% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "AVERAGE PERCENTAGE OVER ALL FILES: 56.49%\n",
      "********************************************************************************\n",
      "TEMPERATURE: 1.2\n",
      "sample_41-50-57-65-69.mid: 51.54% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_82.mid: 73.85% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_39-48-55-63-67.mid: 67.83% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_80.mid: 96.21% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_40-49-56-64-68.mid: 35.86% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_81.mid: 90.25% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "AVERAGE PERCENTAGE OVER ALL FILES: 69.26%\n",
      "********************************************************************************\n",
      "TEMPERATURE: 1.5\n",
      "sample_41-50-57-65-69.mid: 80.89% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_82.mid: 65.88% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_39-48-55-63-67.mid: 59.49% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_80.mid: 40.16% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_40-49-56-64-68.mid: 56.99% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_81.mid: 24.47% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "AVERAGE PERCENTAGE OVER ALL FILES: 54.65%\n",
      "********************************************************************************\n",
      "TEMPERATURE: 2.0\n",
      "sample_41-50-57-65-69.mid: 22.97% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_82.mid: 16.74% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_39-48-55-63-67.mid: 4.18% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_80.mid: 0.42% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_40-49-56-64-68.mid: 19.51% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "sample_81.mid: 66.83% of unique windows (length = 100) from the generated composition exist in the training data.\n",
      "AVERAGE PERCENTAGE OVER ALL FILES: 21.77%\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for temp in temperatures:\n",
    "    print('TEMPERATURE: {}'.format(temp))\n",
    "    \n",
    "    percentages = []\n",
    "\n",
    "    # compare each generated sequence against the training set\n",
    "    for file in Path(generated_data.format(temp)).glob('*.mid'):\n",
    "        \n",
    "        sequence = serializer.serialize(file)\n",
    "            \n",
    "        # split generated sequence into subsequences\n",
    "        gen_sequences, _ = sequence_utils.window([sequence], window_size=window_size)\n",
    "\n",
    "        # create a set of unique subsequences\n",
    "        gen_set = utils.create_hashable_set(gen_sequences)\n",
    "\n",
    "        # find the intersection of the two sets to find all matching subsequences and calculate percentage of generated subsequences that come from the training data\n",
    "        matches = gen_set.intersection(real_set)\n",
    "        n_matches = len(matches)\n",
    "        total = len(gen_set)\n",
    "        percentage = n_matches/total * 100\n",
    "        percentages.append(percentage)\n",
    "\n",
    "        # print results\n",
    "        print('{}: {:.2f}% of unique windows (length = {}) from the generated composition exist in the training data.'.format(file.name, percentage, window_size))\n",
    "                        \n",
    "    average = sum(percentages) / len(percentages)\n",
    "    print('AVERAGE PERCENTAGE OVER ALL FILES: {:.2f}%'.format(average))\n",
    "    print('*' * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How often does Chopin repeat himself?\n",
    "\n",
    "Each composition of the dataset is checked against the rest of the dataset and its transpositions, as above, to find how much repetition appears across compositions in the dataset itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nocturne op15 n3.mid: 0.05% of unique windows from the composition exist in the other training data compositions.\n",
      "Nocturne op27 n1.mid: 0.02% of unique windows from the composition exist in the other training data compositions.\n",
      "Matches found between 2/18 compositions.\n"
     ]
    }
   ],
   "source": [
    "# create a hashable set for each composition in the dataset\n",
    "training_data_sets = list()\n",
    "for file in Path(training_data).glob('*.mid'):\n",
    "    # serialize file\n",
    "    sequence = serializer.serialize(file)\n",
    "    \n",
    "    # split the sequence into subsequences and create a hashable set from the composition\n",
    "    sequences, _ = sequence_utils.window([sequence], window_size=window_size)\n",
    "    hashable_set = utils.create_hashable_set(sequences)\n",
    "    \n",
    "    # transpose the sequence into all keys and create a hashable set from the composition and all its transpositions\n",
    "    sequences = sequence_utils.transpose([sequence], down=-6, up=5)\n",
    "    sequences, _ = sequence_utils.window(sequences, window_size=window_size)\n",
    "    hashable_set_trans = utils.create_hashable_set(sequences)\n",
    "    \n",
    "    # add to list of hashable sets\n",
    "    training_data_sets.append((file, hashable_set, hashable_set_trans))\n",
    "\n",
    "# compare the hashable set for each composition to the union of hashable sets for all other compositions\n",
    "counter = 0\n",
    "for f, hs, _ in training_data_sets:\n",
    "    composition_set = hs\n",
    "    remaining_set = set.union(*[hst for f2, _, hst in training_data_sets if f2 != f])\n",
    "    \n",
    "    # find the intersection of the two sets to find all matching subsequences and calculate percentage of generated subsequences that come from the training data\n",
    "    matches = composition_set.intersection(remaining_set)\n",
    "    n_matches = len(matches)\n",
    "    total = len(composition_set)\n",
    "    percentage = n_matches/total * 100\n",
    "\n",
    "    # print results, if any overlap is found\n",
    "    if percentage > 0.01:\n",
    "        counter += 1\n",
    "        print('{}: {:.2f}% of unique windows from the composition exist in the other training data compositions.'.format(f.name, percentage))\n",
    "\n",
    "print('Matches found between {}/{} compositions.'.format(counter, len(training_data_sets)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When compared to an analysis of the model trained on Bach Chorales, there is significantly more repetition within the Chopin Nocturne model, and significantly less repetition by Chopin within the dataset.\n",
    "\n",
    "This may suggest the training data is too varied to effectively model without ending up with a more significant amount of \"plagiarized\" data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
